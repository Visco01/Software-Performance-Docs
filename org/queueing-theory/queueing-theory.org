#+title: Queueing Theory
#+LATEX_HEADER: \usepackage{amsmath}
#+OPTIONS: tex:mathjax

Queueing theory is the theory behind what happens when you have lots of jobs,
scarce resources, and subsequently long queues and delays. It is literally the
“theory of queues”: what makes queues appear and how to make them go away.

The goals of a queueing theorist are twofold. The first is predicting the
*system performance*. Although prediction is important, an even more important
goal is finding a superior *system design* to improve performance.

* Terminologies

#+begin_quote
*System capacity*: it represents the maximum number of jobs the system can
 contain (both waiting and serviced).
#+end_quote

#+begin_quote
*Population size*: it represents the number of potential jobs that can access
 the system, i.e. they are willing to enter the system.
#+end_quote

#+begin_quote
*Service discipline*: policy through which the jobs are served.
1) *FCFS* (First Come First Serve);
2) *LCFS* (Last Come First Serve);
3) *FCFS/LCFS with preemption*: this feature allows to remove the job from the
   service and putting it back to the queue;
4) *FCFS/LCFS with preemption and resume*: same as previous, but it resumes the
   state of the job (it doesn't waste the work done);
5) *PS* (Processor Sharing): Simplified Round-Robin;
6) *IS* (Delay Center or Infinite Services);
7) *SRPT* (Shortest Remaining Processing Time).
#+end_quote

#+begin_quote
*Work-conserving discipline*: a queueing discipline is work-conserving if:
1) It never leaves idle a server that is allowed to work;
2) It never wastes the work done on a job.
#+end_quote

* Kendall notation
It's a fast way to represent queueing systems.

#+begin_quote
$A/S/m/B/K/SD$

- $A$: inter-arrival distribution
- $S$: service distribution
- $m$: number of servers
- $B$: system capacity (default: $\infty$)
- $K$: population size (default: $\infty$)
- $SD$: service discipline (default: FCFS)
#+end_quote

Abbreviations for $A$ and $M$:
- $M$: exponential (Poisson);
- $D$: deterministic;
- $G$: general distribution.

* M/M/1 queueing system
The M/M/1 open model begins with the following assumptions:
- An average arrival rate $\lambda$ is known;
- The system is running under an stable condition, i.e. $\lambda < \mu$;

*M/M/1* terminology indicates these features:
- Exponential independent inter-arrival times (Poisson arrivals);
- Exponential independent service times;
- Single server, infinite buffer, FCFS discipline.

** Load factor
The load factor is defined as follows: $$\rho=\frac{\lambda}{\mu}$$ and it
represents the ratio between the arrival rate and the service rate.

We already observed that the queue is stable, i.e. $\rho < 1$.

#+begin_quote
The steady-state probability of observing n jobs in the queue has a *geometric
distribution* with ratio $\rho$.
#+end_quote

** Expected number of customers
We have that $$\overline{N}=\frac{\rho}{1-\rho}$$
And the steady-state probability to find the server busy (i.e. number of customers in the service room) is $$U=\rho=E[N_{s}]$$
Recall that $E[N]=E[N_{s}]+E[N_{q}]$, therefore $$E[N_{q}]=\frac{\rho}{1-\rho}-\rho=\frac{\rho^{2}}{1-\rho}$$

** Expected response time
In stability conditions, we have $X = \lambda$.
We can compute the expected response time by Little’s theorem: $$\overline{R}=E[R]=\frac{\overline{N}}{X}=\frac{1}{\mu-\lambda}$$
We know that $E[S]=\mu^{-1}=\frac{1}{\mu}$, therefore $$E[W]=E[R]-E[S]=\frac{1}{\mu-\lambda}-\frac{1}{\mu}=\frac{\lambda}{\mu(\mu-\lambda)}$$

** M/M/1 charts
[[../../resources/mm1-ncustomers.png]]

#+CAPTION: This chart can be subdivided in three parts: Low Load, Moderate Load, Heavy Load. The last one MUST be avoided due to the exponential response time based on little variations of \lambda.
[[../../resources/mm1-response-time.png]]

** Other useful relations in M/M/1
#+begin_quote
*Probability to find the empty queue*: $1-\frac{\lambda}{\mu}=1-\rho$
#+end_quote

#+begin_quote
*Length of idle periods in the system*: exponentially distributed with mean
 $\frac{1}{\lambda}$
#+end_quote

#+begin_quote
*Average length of busy periods*: $X=\frac{1}{\mu-\lambda}$
#+end_quote

* M/M/m queueing system
In today’s high-volume world, almost no websites, compute centers, or call
centers consist of just a single server. Instead a “server farm” is used. The
server farm is a collection of servers that work together to handle incoming
requests. Each request might be routed to a different server, so that servers
“share” the incoming load. From a practical perspective, server farms are often
preferable to a single “super-fast” server because of their low cost (many slow
servers are cheaper than a single fast one) and their flexibility (it is easy to
increase/decrease capacity as needed by adding/removing servers).

The first “M” indicates that we have *memoryless interarrival times*, and the
second “M” indicates *memoryless service times*. The third field denotes that *m
servers share a common pool* of arriving jobs. For the M/M/k system, there is no
capacity constraint

[[../../resources/mmm-queue.png]]

Since the incoming stream is completely independent from the system, we denote $$\lambda(n)=\lambda$$
and the service rate is based on the number of $n$ busy servers

$$\mu(n) =
\begin{cases}
  n\mu & \text{n <= m} \\
  m\mu & \text{otherwise}
\end{cases}$$

** Load factor
In this type of queue system the load factor is defined as follows: $$\rho=\frac{\lambda}{m\mu}$$
Therefore, the system is stable if and only if $\lambda < m\mu$

** Expected number of customers
Let $C(m,\frac{\lambda}{\mu})$ be the probability of finding all the servers busy.

Assuming stability we have that $\lambda = X$ where $X$ is the throughput, and the expected number of customers is derived as follows

$$\overline{N}=E[N]=\frac{\rho}{1-\rho}C(m,\frac{\lambda}{\mu})+\frac{\lambda}{\mu}$$

** Expected response time
$\overline{R}$ is derived by Little's theorem as follows $$\overline{R}=E[R]=\frac{C(m,\frac{\lambda}{\mu})}{m\mu-\lambda}+\frac{1}{\mu}$$

** M/M/m charts

[[../../resources/mmm-ncustomers.png]]

[[../../resources/mmm-response-time.png]]
